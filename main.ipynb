{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import netCDF4 as nc\n",
    "from netCDF4 import Dataset\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.stats import gaussian_kde, poisson\n",
    "import tropycal.tracks as tracks\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import datetime as dt\n",
    "import os\n",
    "import re\n",
    "import geopandas as gpd\n",
    "import geodatasets\n",
    "from shapely import geometry, wkb, wkt\n",
    "import multiprocessing as mp\n",
    "import sys\n",
    "import threading\n",
    "from queue import Queue\n",
    "import xarray as xr\n",
    "import pathlib\n",
    "import sqlite3\n",
    "import pickle\n",
    "from scipy.stats import mannwhitneyu"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Western Pacific (WP) locations: 5–60°N 100°–180°E"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Current"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s = re.compile('^MRI_HPB_m+\\d{3}.nc$')\n",
    "file_dirs = os.listdir('data')\n",
    "file_dirs\n",
    "# data_files = [f for f in file_dirs if s.match(f)]\n",
    "data_files_dirs = [pathlib.Path('data') / pathlib.Path(f) for f in file_dirs if s.match(f)]\n",
    "# data_files\n",
    "len(data_files_dirs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_storms = xr.open_dataset(data_files_dirs[1])\n",
    "i = 0\n",
    "test_storms.track_lat[i]\n",
    "# for lat in test_storms.track_lat[i]:\n",
    "#     print(lat.values)\n",
    "test_storms.where((test_storms.track_lat >= 5) & (test_storms.track_lat <= 60) & (test_storms.track_lon >= 100) & (test_storms.track_lon <= 180), drop=True)\n",
    "# np.isnat(test_storms.track_time)\n",
    "# t = geometry.LineString(zip(test_storms.track_lon[i], test_storms.track_lat[i]))\n",
    "# b = t.wkb\n",
    "# b.hex()\n",
    "# tb = wkb.loads(b, hex=True)\n",
    "# tb\n",
    "# np.datetime_as_string(test_storms.track_time[1][3].values, unit='m')\n",
    "# data_files_dirs[1].stem\n",
    "# type(test_storms.track_time[0].values[0])\n",
    "test_storms.track_time[0].values[0] >= np.datetime64('2000-01-01')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# storm_geo = mp.Manager().list()\n",
    "def create_storm_dict_task(storm_i, storms_d, storms, stem):\n",
    "    time_l = storms.track_time[storm_i].where(~np.isnat(storms.track_time[storm_i]), drop=True).values\n",
    "    lon_l = storms.track_lon[storm_i].where(~np.isnan(storms.track_lon[storm_i]), drop=True).values\n",
    "    lat_l = storms.track_lat[storm_i].where(~np.isnan(storms.track_lat[storm_i]), drop=True).values\n",
    "    pres_l = storms.track_pres[storm_i].where(~np.isnan(storms.track_pres[storm_i]), drop=True).values\n",
    "    wind_l = storms.track_wind[storm_i].where(~np.isnan(storms.track_wind[storm_i]), drop=True).values\n",
    "    # for i in range(len(storms.track_time[storm_i])):\n",
    "    #     # lat = storms.track_lat[storm_i][i]\n",
    "    #     # lon = storms.track_lon[storm_i][i]\n",
    "    #     if not np.isnan(storms.track_lon[storm_i][i]):\n",
    "    #         coords.append((storms.track_lon[storm_i][i], storms.track_lat[storm_i][i]))\n",
    "    #     else:\n",
    "    #         break\n",
    "    if len(lat_l) <= 1:\n",
    "        # print(f'{stem}_{storm_i} has no valid coordinates')\n",
    "        return\n",
    "    storms_d[f'{stem}_{storm_i}'] = {\n",
    "        # 'time': storms[i]['time'],\n",
    "        # 'geometry': geometry.LineString((lat, lon) for lat, lon in zip(storms.track_lon[storm_i], storms.track_lat[storm_i]) if (not np.isnan(lat) and not np.isnan(lon))),\n",
    "        # 'geometry': geometry.LineString(coords),\n",
    "        # 'featurecla': 'storm',\n",
    "        # 'scalerank': 1,\n",
    "        # 'min_zoom': 1,\n",
    "        'time': time_l,\n",
    "        'lat': lat_l,\n",
    "        'lon': lon_l,\n",
    "        'pres': pres_l,\n",
    "        'wind': wind_l\n",
    "        \n",
    "    }\n",
    "    # geometry_d.append(geometry.LineString((lat, lon) for lat, lon in zip(storms.track_lon[storm_i], storms.track_lat[storm_i]) if (not np.isnan(lat) and not np.isnan(lon))))\n",
    "    # print(storm_i)\n",
    "def generate_storm_dict(stem, storms_d):\n",
    "    storms = xr.open_dataset(f'data/{stem}.nc')\n",
    "    filtered_storms = storms.where((storms.track_lat >= 5) & (storms.track_lat <= 60) & (storms.track_lon >= 100) & (storms.track_lon <= 180) & (storms.track_time >= np.datetime64('2000-01-01')), drop=True)\n",
    "    # filtered_storms = storms.where((storms.track_lat >= 5) & (storms.track_lat <= 60) & (storms.track_lon >= 60) & (storms.track_lon <= 280), drop=True)\n",
    "    # print(filtered_storms)\n",
    "    print(f\"{stem}\\n\", end='')\n",
    "    for storm_i in range(len(filtered_storms.storm)):\n",
    "        create_storm_dict_task(storm_i, storms_d, filtered_storms, stem)\n",
    "    # pool = mp.Pool(mp.cpu_count())\n",
    "    # pool.starmap(create_storm_dict_task, ((storm_i, storms_d, filtered_storms, stem) for storm_i in range(len(filtered_storms.storm))))\n",
    "    # pool.close()\n",
    "    # pool.join()\n",
    "# processes = []\n",
    "# for data in data_files_dirs:\n",
    "#     storms = xr.open_dataset(data)\n",
    "#     generate_storm_dict(storms, data.stem)\n",
    "\n",
    "\n",
    "    # processes.append(mp.Process(target=generate_geometry, args=(storms, data.stem)))\n",
    "    # generate_geometry(storms, data.stem)\n",
    "# for p in processes:\n",
    "#     p.start()\n",
    "# for p in processes:\n",
    "#     p.join()\n",
    "# storm_geo = storm_geo._getvalue()\n",
    "# print(storm_geo)\n",
    "# generate_geometry(test_storms, \"test\")\n",
    "# create_geometry(1203, storm_geo, test_storms, \"test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "storms_d = mp.Manager().dict()\n",
    "pool = mp.Pool(mp.cpu_count())\n",
    "pool.starmap(generate_storm_dict, ([data.stem, storms_d] for data in data_files_dirs))\n",
    "pool.close()\n",
    "pool.join()\n",
    "storms_d = storms_d._getvalue()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('storms_d.p', 'wb') as f:\n",
    "    pickle.dump(storms_d, f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('storms_d.p', 'rb') as f:\n",
    "    storms_d = pickle.load(f)\n",
    "storms_d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# world = gpd.read_file(geodatasets.get_path('naturalearth.land'))\n",
    "# df = pd.DataFrame.from_dict(storms_d, orient='index')\n",
    "# roi = gpd.GeoDataFrame(df, geometry='geometry', crs=world.crs)\n",
    "# roi\n",
    "# world_roi = pd.concat([world, roi])\n",
    "# world_roi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def createGrid(storms, n, res):\n",
    "    storms_df = pd.DataFrame.from_dict(storms, orient='index')\n",
    "    lat_max = storms_df['lat'].apply(np.max).max()\n",
    "    lat_min = storms_df['lat'].apply(np.min).min()\n",
    "    lon_max = storms_df['lon'].apply(np.max).max()\n",
    "    lon_min = storms_df['lon'].apply(np.min).min()\n",
    "    time_min = storms_df['time'].apply(np.min).min()\n",
    "    time_max = storms_df['time'].apply(np.max).max()\n",
    "    year_num = (time_max-time_min).days/365.2425\n",
    "    grids = np.zeros((round((lon_max-lon_min)/res)+1, round((lat_max-lat_min)/res)+1))\n",
    "    for i in storms:\n",
    "        # cell_coords = {}\n",
    "        coords = []\n",
    "        for lat, lon in zip(storms[i]['lat'], storms[i]['lon']):\n",
    "                # grids[round((lon-lon_min)/res), round((lat-lat_min)/res)] += 1\n",
    "                if (round((lon-lon_min)/res), round((lat-lat_min)/res)) not in coords:\n",
    "                    coords.append((round((lon-lon_min)/res), round((lat-lat_min)/res)))\n",
    "        for i, j in coords:\n",
    "            grids[i, j] += 1\n",
    "                \n",
    "    grids = grids/year_num * 10/n\n",
    "    # grid_cells = []\n",
    "    X = []\n",
    "    Y = []\n",
    "    C = np.zeros((grids.shape[1], grids.shape[0]))\n",
    "\n",
    "    for i in range(grids.shape[0]):\n",
    "        X.append(lon_min + (i+0.5)*res)\n",
    "    for i in range(grids.shape[1]):\n",
    "        Y.append(lat_min + (i+0.5)*res)\n",
    "    for i in range(grids.shape[0]):\n",
    "        for j in range(grids.shape[1]):\n",
    "            C[j, i] = grids[i, j]\n",
    "            \n",
    "    lon_mesh, lat_mesh = np.meshgrid(X, Y)\n",
    "    return lon_mesh, lat_mesh, C, lat_min, lat_max, lon_min, lon_max\n",
    "\n",
    "def plot_map(lon_mesh, lat_mesh, C, lat_min, lat_max, lon_min, lon_max):\n",
    "    world = gpd.read_file(geodatasets.get_path('naturalearth.land'))\n",
    "    norm = plt.matplotlib.colors.Normalize(vmin=C.min(), vmax=C.max())\n",
    "    fig, ax = plt.subplots()\n",
    "    world.plot(color='gray', edgecolor='black', ax=ax)\n",
    "    ax.figure.set_size_inches(20, 15)\n",
    "    ax.set_xlim(100, 180)\n",
    "    ax.set_ylim(5, 60)\n",
    "    cmap = plt.get_cmap('rainbow', 256)\n",
    "    cmap.set_bad((0, 0, 0, 0))\n",
    "    colors = cmap(np.linspace(0, 1, 256))\n",
    "    # colors[0][3] = 0\n",
    "    cmap = plt.matplotlib.colors.ListedColormap(colors)\n",
    "    pcm = ax.pcolormesh(lon_mesh, lat_mesh, C, cmap=cmap, shading='auto', alpha=0.8, norm=norm)\n",
    "    num_ticks = 6\n",
    "    cbar = fig.colorbar(pcm, ax=ax, orientation='vertical',\n",
    "                        extend='max',\n",
    "                        ticks=[round(i, 2) for i in np.linspace(norm.vmin, norm.vmax, num_ticks)],\n",
    "                        pad=0.02, \n",
    "                        fraction=0.03)\n",
    "    cbar.ax.set_title('[No./10yr]')\n",
    "    cbar.ax.set_yticklabels([f'{round(i, 2)}' for i in np.linspace(norm.vmin, norm.vmax, num_ticks)])\n",
    "    plt.tight_layout()\n",
    "\n",
    "def plot(storms, n, res):\n",
    "    # res = 2.25\n",
    "    # res = 1.\n",
    "    world = gpd.read_file(geodatasets.get_path('naturalearth.land'))\n",
    "    # storms = storms_d\n",
    "    \n",
    "    lon_mesh, lat_mesh, C, lat_min, lat_max, lon_min, lon_max = createGrid(storms, n, res)\n",
    "    \n",
    "    norm = plt.matplotlib.colors.LogNorm(vmin=0.1, vmax=C.max())\n",
    "    fig, ax = plt.subplots()\n",
    "    world.plot(color='gray', edgecolor='black', ax=ax)\n",
    "    ax.figure.set_size_inches(20, 15)\n",
    "    ax.set_xlim(100, 180)\n",
    "    ax.set_ylim(5, 60)\n",
    "    cmap = plt.get_cmap('rainbow', 256)\n",
    "    colors = cmap(np.linspace(0, 1, 256))\n",
    "    colors[0][3] = 0\n",
    "    cmap = plt.matplotlib.colors.ListedColormap(colors)\n",
    "    pcm = ax.pcolormesh(lon_mesh, lat_mesh, C, cmap=cmap, shading='auto', alpha=0.8, norm=norm)\n",
    "    num_ticks = 6\n",
    "    cbar = fig.colorbar(pcm, ax=ax, orientation='vertical',\n",
    "                        extend='max',\n",
    "                        ticks=[round(i, 2) for i in np.linspace(norm.vmin, norm.vmax, num_ticks)],\n",
    "                        pad=0.02, \n",
    "                        fraction=0.03)\n",
    "    cbar.ax.set_title('[No./10yr]')\n",
    "    cbar.ax.set_yticklabels([f'{round(i, 2)}' for i in np.linspace(norm.vmin, norm.vmax, num_ticks)])\n",
    "    plt.tight_layout()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot(storms_d, 100, 2.25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s = re.compile('^MRI_HFB')\n",
    "file_dirs = os.listdir('data')\n",
    "file_dirs\n",
    "# data_files = [f for f in file_dirs if s.match(f)]\n",
    "data_files_dirs = [pathlib.Path('data') / pathlib.Path(f) for f in file_dirs if s.match(f)]\n",
    "# data_files\n",
    "len(data_files_dirs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "storms_4K_d = mp.Manager().dict()\n",
    "pool = mp.Pool(mp.cpu_count())\n",
    "pool.starmap(generate_storm_dict, ([data.stem, storms_4K_d] for data in data_files_dirs))\n",
    "pool.close()\n",
    "pool.join()\n",
    "storms_4K_d = storms_4K_d._getvalue()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('storms_4K_d.p', 'wb') as f:\n",
    "    pickle.dump(storms_4K_d, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('storms_4K_d.p', 'rb') as f:\n",
    "    storms_4K_d = pickle.load(f)\n",
    "storms_4K_d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot(storms_4K_d, 90, 2.25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "obs = xr.open_dataset('data/IBTrACS.WP.v04r01.nc')\n",
    "obs = obs.where((obs.lat >= 5) & (obs.lat <= 60) & (obs.lon >= 100) &  (obs.lon <= 180), drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "storms_obs_d = {}\n",
    "for i in range(len(obs.storm)):\n",
    "    time = obs.time[i].where(~np.isnan(obs.usa_lat[i]), drop=True).values\n",
    "    # lat = obs.usa_lat[i].where(~np.isnan(obs.usa_lat[i]), drop=True).values\n",
    "    # lon = obs.usa_lon[i].where(~np.isnan(obs.usa_lon[i]), drop=True).values\n",
    "    lat = obs.lat[i].where(~np.isnan(obs.lat[i]), drop=True).values\n",
    "    lon = obs.lon[i].where(~np.isnan(obs.lon[i]), drop=True).values\n",
    "    if len(time) != 0:\n",
    "        storms_obs_d[f'IBTrACS_{i}'] = {\n",
    "            'time': time,\n",
    "            'lat': lat,\n",
    "            'lon': lon,\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('storms_obs_d.p', 'wb') as f:\n",
    "    pickle.dump(storms_obs_d, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('storms_obs_d.p', 'rb') as f:\n",
    "    pickle.load(f)\n",
    "storms_obs_d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot(storms_obs_d, 1, 2.25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s = re.compile('^MRI_HPB_m+\\d{3}.nc$')\n",
    "file_dirs = os.listdir('data')\n",
    "file_dirs\n",
    "# data_files = [f for f in file_dirs if s.match(f)]\n",
    "data_files_dirs = [pathlib.Path('data') / pathlib.Path(f) for f in file_dirs if s.match(f)]\n",
    "# data_files\n",
    "len(data_files_dirs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "current_d_keys = storms_d.keys()\n",
    "current_storms_df = pd.DataFrame.from_dict(storms_d, orient='index')\n",
    "lat_max = current_storms_df['lat'].apply(np.max).max()\n",
    "lat_min = current_storms_df['lat'].apply(np.min).min()\n",
    "lon_max = current_storms_df['lon'].apply(np.max).max()\n",
    "lon_min = current_storms_df['lon'].apply(np.min).min()\n",
    "time_min = current_storms_df['time'].apply(np.min).min()\n",
    "time_max = current_storms_df['time'].apply(np.max).max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_current = np.empty((round((lon_max-lon_min)/2.25)+1, round((lat_max-lat_min)/2.25)+1), dtype=list)\n",
    "grid_current[:] = [[[] for _ in range(grid_current.shape[1])] for _ in range(grid_current.shape[0])]\n",
    "for current_data_i, current_data in enumerate(data_files_dirs):\n",
    "    stem = current_data.stem\n",
    "    re_exp = re.compile(f'^{stem}.+')\n",
    "    filtered_key = [i for i in current_d_keys if re_exp.match(i)]\n",
    "    temp_grid = np.zeros((round((lon_max-lon_min)/2.25)+1, round((lat_max-lat_min)/2.25)+1))\n",
    "    for key in filtered_key:\n",
    "        coords = []\n",
    "        for lat, lon in zip(storms_d[key]['lat'], storms_d[key]['lon']):\n",
    "            if (round((lon-lon_min)/2.25), round((lat-lat_min)/2.25)) not in coords:\n",
    "                coords.append((round((lon-lon_min)/2.25), round((lat-lat_min)/2.25)))\n",
    "        for i, j in coords:\n",
    "            temp_grid[i, j] += 1\n",
    "    for i in range(temp_grid.shape[0]):\n",
    "        for j in range(temp_grid.shape[1]):\n",
    "            if temp_grid[i, j] > 0:\n",
    "                grid_current[i, j].append(temp_grid[i, j])\n",
    "grid_current"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s = re.compile('^MRI_HFB')\n",
    "file_dirs = os.listdir('data')\n",
    "file_dirs\n",
    "# data_files = [f for f in file_dirs if s.match(f)]\n",
    "future_data_files_dirs = [pathlib.Path('data') / pathlib.Path(f) for f in file_dirs if s.match(f)]\n",
    "# data_files\n",
    "len(future_data_files_dirs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "future_d_keys = storms_4K_d.keys()\n",
    "future_storms_df = pd.DataFrame.from_dict(storms_4K_d, orient='index')\n",
    "lat_max = future_storms_df['lat'].apply(np.max).max()\n",
    "lat_min = future_storms_df['lat'].apply(np.min).min()\n",
    "lon_max = future_storms_df['lon'].apply(np.max).max()\n",
    "lon_min = future_storms_df['lon'].apply(np.min).min()\n",
    "time_min = future_storms_df['time'].apply(np.min).min()\n",
    "time_max = future_storms_df['time'].apply(np.max).max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_future = np.empty((round((lon_max-lon_min)/2.25)+1, round((lat_max-lat_min)/2.25)+1), dtype=list)\n",
    "grid_future[:] = [[[] for _ in range(grid_future.shape[1])] for _ in range(grid_future.shape[0])]\n",
    "for future_data_i, future_data in enumerate(future_data_files_dirs):\n",
    "    stem = future_data.stem\n",
    "    re_exp = re.compile(f'^{stem}.+')\n",
    "    filtered_key = [i for i in future_d_keys if re_exp.match(i)]\n",
    "    temp_grid = np.zeros((round((lon_max-lon_min)/2.25)+1, round((lat_max-lat_min)/2.25)+1))\n",
    "    for key in filtered_key:\n",
    "        coords = []\n",
    "        for lat, lon in zip(storms_4K_d[key]['lat'], storms_4K_d[key]['lon']):\n",
    "            if (round((lon-lon_min)/2.25), round((lat-lat_min)/2.25)) not in coords:\n",
    "                coords.append((round((lon-lon_min)/2.25), round((lat-lat_min)/2.25)))\n",
    "        for i, j in coords:\n",
    "            temp_grid[i, j] += 1\n",
    "    for i in range(temp_grid.shape[0]):\n",
    "        for j in range(temp_grid.shape[1]):\n",
    "            if temp_grid[i, j] > 0:\n",
    "                grid_future[i, j].append(temp_grid[i, j])\n",
    "grid_future"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid = np.zeros((grid_current.shape[0], grid_current.shape[1]))\n",
    "for i in range(grid.shape[0]):\n",
    "    for j in range(grid.shape[1]):\n",
    "        try:\n",
    "            stat, p = mannwhitneyu(grid_future[i, j], grid_current[i, j])\n",
    "            if p < 0.01:\n",
    "                grid[i, j] = np.mean(grid_future[i, j]) - np.mean(grid_current[i, j])\n",
    "        except:\n",
    "            # print(grid_current[i, j], grid_future[i, j])\n",
    "            pass\n",
    "grid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid = grid / (time_max-time_min).days * 365.2425 * 10\n",
    "grid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = []\n",
    "Y = []\n",
    "for i in range(grid.shape[0]):\n",
    "    X.append(lon_min + (i+0.5)*2.25)\n",
    "for i in range(grid.shape[1]):\n",
    "    Y.append(lat_min + (i+0.5)*2.25)\n",
    "lon_mesh, lat_mesh = np.meshgrid(X, Y)\n",
    "grid = np.ma.masked_equal(grid, 0)\n",
    "plot_map(lon_mesh, lat_mesh, grid.T, lat_min, lat_max, lon_min, lon_max)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
